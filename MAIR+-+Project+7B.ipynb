{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are creating a dataset which is based on all of the dual-task trials per block and we are ignoring the practice trials\n",
    "The dataset is built by merging the results acquired in the paper created by Chris.\n",
    "\n",
    "Some columns are based on the significance provided in the said paper ( & maybe image here)\n",
    "The csv files we are merging into our dataset are provided below\n",
    "\n",
    "- MaxDeviationPerTrial.csv\n",
    "- maxnrDigitEnteredPerPPAndPerBlockPerTrial.csv\n",
    "- numberOfVisitsTrackerPerParticipantPerBlock.csv (trial data)\n",
    "- samplesOutsideTrial.csv(*)\n",
    "- sdVisTime.csv(*)\n",
    "- tableForMeanTimeInDigitPerPPandPerBlock.csv(*) - merge the data ber block maybe?\n",
    "- We are using the payoff function values provided in the meanVisTime.csv file\n",
    "- The columns marked with star could be significant and we are testing to see the results with or without the said feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialNumber</th>\n",
       "      <th>PayOffFunction</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Noise</th>\n",
       "      <th>IKINumber</th>\n",
       "      <th>MeanDeviation</th>\n",
       "      <th>MaxDeviation</th>\n",
       "      <th>MeanNrDigits</th>\n",
       "      <th>MaxNrDigits</th>\n",
       "      <th>MeanTimeTypingWindow</th>\n",
       "      <th>MaxTimeTypingWindow</th>\n",
       "      <th>MeanTrackingTime</th>\n",
       "      <th>MaxTrackingTime</th>\n",
       "      <th>TrackingWindowVisitCounter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>31.836899</td>\n",
       "      <td>76.66</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2.079050</td>\n",
       "      <td>2.8536</td>\n",
       "      <td>1.331467</td>\n",
       "      <td>1.6520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>16.518333</td>\n",
       "      <td>44.27</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>9</td>\n",
       "      <td>2.111167</td>\n",
       "      <td>2.3783</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>1.0343</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>26.661194</td>\n",
       "      <td>69.16</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>2.060633</td>\n",
       "      <td>2.4747</td>\n",
       "      <td>1.342900</td>\n",
       "      <td>1.3780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>36.799188</td>\n",
       "      <td>71.33</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>3.286850</td>\n",
       "      <td>3.3149</td>\n",
       "      <td>1.582600</td>\n",
       "      <td>1.5826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>15.096851</td>\n",
       "      <td>37.37</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>2.154425</td>\n",
       "      <td>3.6314</td>\n",
       "      <td>0.989533</td>\n",
       "      <td>1.1029</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>112</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>27.756481</td>\n",
       "      <td>66.45</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>1.934700</td>\n",
       "      <td>3.3108</td>\n",
       "      <td>1.377100</td>\n",
       "      <td>1.5838</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>113</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>27.953079</td>\n",
       "      <td>69.58</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.928625</td>\n",
       "      <td>4.0674</td>\n",
       "      <td>1.331167</td>\n",
       "      <td>1.4453</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>14.232909</td>\n",
       "      <td>45.58</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1.628350</td>\n",
       "      <td>2.4539</td>\n",
       "      <td>1.011167</td>\n",
       "      <td>1.1714</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>115</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>29.031232</td>\n",
       "      <td>70.15</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>2.936750</td>\n",
       "      <td>2.9834</td>\n",
       "      <td>1.308100</td>\n",
       "      <td>1.3081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>116</td>\n",
       "      <td>Speed</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>30.585000</td>\n",
       "      <td>63.01</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>9</td>\n",
       "      <td>2.193600</td>\n",
       "      <td>2.6000</td>\n",
       "      <td>1.172250</td>\n",
       "      <td>1.2398</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TrialNumber PayOffFunction  Radius  Noise  IKINumber  MeanDeviation  \\\n",
       "0          107          Speed      80      3   0.308554      31.836899   \n",
       "1          108          Speed      80      3   0.308554      16.518333   \n",
       "2          109          Speed      80      3   0.308554      26.661194   \n",
       "3          110          Speed      80      3   0.308554      36.799188   \n",
       "4          111          Speed      80      3   0.308554      15.096851   \n",
       "5          112          Speed      80      3   0.308554      27.756481   \n",
       "6          113          Speed      80      3   0.308554      27.953079   \n",
       "7          114          Speed      80      3   0.308554      14.232909   \n",
       "8          115          Speed      80      3   0.308554      29.031232   \n",
       "9          116          Speed      80      3   0.308554      30.585000   \n",
       "\n",
       "   MaxDeviation  MeanNrDigits  MaxNrDigits  MeanTimeTypingWindow  \\\n",
       "0         76.66      5.000000            6              2.079050   \n",
       "1         44.27      6.666667            9              2.111167   \n",
       "2         69.16      6.666667            8              2.060633   \n",
       "3         71.33     10.000000           11              3.286850   \n",
       "4         37.37      5.000000            7              2.154425   \n",
       "5         66.45      5.000000            9              1.934700   \n",
       "6         69.58      5.000000           10              1.928625   \n",
       "7         45.58      5.000000            7              1.628350   \n",
       "8         70.15     10.000000           11              2.936750   \n",
       "9         63.01      6.666667            9              2.193600   \n",
       "\n",
       "   MaxTimeTypingWindow  MeanTrackingTime  MaxTrackingTime  \\\n",
       "0               2.8536          1.331467           1.6520   \n",
       "1               2.3783          1.001200           1.0343   \n",
       "2               2.4747          1.342900           1.3780   \n",
       "3               3.3149          1.582600           1.5826   \n",
       "4               3.6314          0.989533           1.1029   \n",
       "5               3.3108          1.377100           1.5838   \n",
       "6               4.0674          1.331167           1.4453   \n",
       "7               2.4539          1.011167           1.1714   \n",
       "8               2.9834          1.308100           1.3081   \n",
       "9               2.6000          1.172250           1.2398   \n",
       "\n",
       "   TrackingWindowVisitCounter  \n",
       "0                           3  \n",
       "1                           2  \n",
       "2                           2  \n",
       "3                           1  \n",
       "4                           3  \n",
       "5                           3  \n",
       "6                           3  \n",
       "7                           3  \n",
       "8                           1  \n",
       "9                           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loaded the data!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We first read the data and display it\n",
    "from sklearn.utils import shuffle\n",
    "final_rows = []\n",
    "\n",
    "output_dist = 'distance'\n",
    "output_pred = 'prediction'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test = False\n",
    "\n",
    "data = pd.read_csv('./data.csv');\n",
    "\n",
    "if test:\n",
    "    data = shuffle(data)\n",
    "    data = data.head(500)\n",
    "\n",
    "\n",
    "display(data.head(10))\n",
    "\n",
    "display('Loaded the data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into 8 data sets\n",
    "We first split the data into 8 datasets and we calculate the mean value for each of the datasetin order to see how biased the data is in terms of Radius, Noise and the payoff function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting length : 8\n"
     ]
    }
   ],
   "source": [
    "split_on = ['PayOffFunction','Radius','Noise']\n",
    "\n",
    "def split(dataSets,col):\n",
    "    \n",
    "    s = []\n",
    "    \n",
    "    for data in dataSets:\n",
    "        \n",
    "        if(col >= len(split_on)):\n",
    "            return None\n",
    "        \n",
    "        values = {}\n",
    "        \n",
    "        for row in range(0,data.shape[ 0 ]):\n",
    "            splitColumn = split_on[ col ]\n",
    "\n",
    "            rowValue = data.iloc[ row ][ splitColumn ]\n",
    "            \n",
    "            if not rowValue in values:\n",
    "                values[ rowValue ] = [ ]\n",
    "                \n",
    "            values[ rowValue ].append(row)\n",
    "            \n",
    "        for value in values:\n",
    "            nextSet = []\n",
    "            \n",
    "            for row in values[value]:\n",
    "                nextSet.append(data.iloc[ row ])\n",
    "                \n",
    "            s.append(pd.DataFrame(nextSet))\n",
    "    res = split( s , col + 1 )\n",
    "    \n",
    "    if res is None:\n",
    "        return s\n",
    "    else: \n",
    "        return res\n",
    "    \n",
    "sets = [ data ]\n",
    "\n",
    "split_sets = split(sets,0)\n",
    "\n",
    "#for debug\n",
    "#i = 0\n",
    "#for item in split_sets:\n",
    "#    item.to_csv('dataset_' + str(i) + '.csv')\n",
    "#    i = i + 1\n",
    "\n",
    "print('resulting length : ' +  str(len(split_sets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#We then take the top N percent of each of the sets\n",
    "\n",
    "def get_by_split(sets,start, step, end, func):\n",
    "    \n",
    "    while start <= end:\n",
    "    \n",
    "        for dataSet in sets:\n",
    "            func(start,dataSet)\n",
    "            \n",
    "        start = start + step\n",
    "\n",
    "per_tick = {}\n",
    "\n",
    "#we save all of the datasets in this variable for later use\n",
    "datasets = []\n",
    "\n",
    "#We then compute the mean for the score column for the provided dataset\n",
    "def mean(current_tick,item):\n",
    "\n",
    "    #meanVal = item['Score'].mean()\n",
    "    \n",
    "    #if current_tick not in per_tick:\n",
    "    #    per_tick[current_tick]= []\n",
    "   #     \n",
    "   # per_tick[current_tick].append(meanVal)\n",
    "    datasets.append(item)\n",
    "\n",
    "get_by_split(split_sets, 1, 0.05, 1,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d = []\n",
    "for dataset in datasets:\n",
    "    newset = shuffle(dataset)\n",
    "    d.append(newset)\n",
    "datasets = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For each of the produced datasets we can now compute different statististics.\n",
    "Currently we are computing and plotting the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now measure the distributions\n",
    "# by testing different sizes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for tick_size in per_tick:\n",
    "    tick_marks = np.arange(len(per_tick[tick_size]))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(tick_marks,per_tick[tick_size])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 8 datasets\n",
      "starting to aquire chunk 0\n",
      "starting to aquire chunk 1\n",
      "starting to aquire chunk 2\n",
      "starting to aquire chunk 3\n",
      "starting to aquire chunk 4\n",
      "starting to aquire chunk 5\n",
      "starting to aquire chunk 6\n",
      "starting to aquire chunk 7\n",
      "starting to aquire chunk 8\n",
      "starting to aquire chunk 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(192, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done partitioning in chunks\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "\n",
    "i = 0;\n",
    "perc = 10\n",
    "rowCounter = 0\n",
    "\n",
    "print( 'there are ' + str(len(datasets)) + ' datasets' )\n",
    "\n",
    "while(i <= 9):\n",
    "    \n",
    "    print('starting to aquire chunk ' + str(i))\n",
    "    \n",
    "    rp = []\n",
    "\n",
    "    for next_set in datasets:\n",
    "        size = int(next_set.shape[0] / perc)\n",
    "        \n",
    "        #starting point\n",
    "        take = i * size\n",
    "        start = take\n",
    "        end = take + size\n",
    "        next_items = next_set[start:end]\n",
    "        \n",
    "        frame = pd.DataFrame(next_items,columns=next_set.columns)\n",
    "        rp.append(frame)\n",
    "        \n",
    "    dictionary[ i ] = pd.concat(shuffle(rp))\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "display(dictionary[0].shape)\n",
    "    \n",
    "print('done partitioning in chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Then we split the data by using cross-sampling and we use cross-validation\n",
    "\n",
    "The idea behind this is that we want to have equal distribution of the data in order to avoid the model becoming biased towards certain characteristics. We also remove some of the columns that are not neccessary when training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "def cross_validate(func, label, ignore_columns):\n",
    " splits = {}\n",
    " k = 0\n",
    " \n",
    " best_score = -999999\n",
    " model_y = None\n",
    " ordered_dataset = None\n",
    " print('Begin training classifier')\n",
    "\n",
    " result_items = []\n",
    "\n",
    " for k in dictionary:\n",
    "     test_set = dictionary[k]\n",
    "     train_q = [test_set]\n",
    "    \n",
    "     for key in dictionary:\n",
    "         if key == k: \n",
    "             continue\n",
    "         train_q.append(dictionary[key])\n",
    "        \n",
    "     train_data = pd.concat(train_q)\n",
    "     re_insert = {}\n",
    "    \n",
    "     for i in ignore_columns:\n",
    "        re_insert[i] = train_data[i]\n",
    "        train_data = train_data.drop(i,1)\n",
    "        \n",
    "     y = train_data[ label ]\n",
    "     X = train_data.drop( label , 1 )\n",
    "     \n",
    "     indic = []\n",
    "    \n",
    "     for i in range(0, X.shape[0]):\n",
    "        if i < test_set.shape[0]:\n",
    "            indic.append(1)\n",
    "        else:\n",
    "            indic.append(-1)\n",
    "                \n",
    "     #https://stackoverflow.com/questions/31948879/using-explict-predefined-validation-set-for-grid-search-with-sklearn\n",
    "     ps = PredefinedSplit(test_fold=indic)\n",
    "     \n",
    "     result = func(X, y, ps)\n",
    "        \n",
    "     for col in ignore_columns:\n",
    "         result[0].X[col]=re_insert[col]\n",
    "    \n",
    "     result_items.append(result[0])\n",
    "     \n",
    " return result_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a support vector classifier and initialize\n",
    "\n",
    "We also create a grid which is used to search for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sv = SVC(cache_size=7000)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dict_grid = {\n",
    "    #sv :  {'kernel' : ['linear'] }\n",
    "    sv: [\n",
    "            {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100]},\n",
    "           {'kernel': ['linear'], 'C': [1, 10, 100]}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. We train the model using the provided grid\n",
    "\n",
    "After the training is finished and we compute the distance to the hyperplane and the classification results for all of the data.\n",
    "We are using the accuracy score to rate the quality of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class estimation:\n",
    "    def __init__(self, model, train_score, test_score, X, y,test_size):\n",
    "        self.test_size = test_size\n",
    "        self.X = X.copy()\n",
    "        self.test_score = test_score\n",
    "        self.train_score = train_score\n",
    "        self.model = model\n",
    "        self.labels = y\n",
    "        self.XOriginal = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(X,y,fold):\n",
    "    \n",
    " models = []\n",
    " predictions = None\n",
    " algorithms = [sv]\n",
    " scores = ['accuracy']\n",
    "    \n",
    " for score in scores:\n",
    "    for algorithm in algorithms:\n",
    "\n",
    "        #display('Training started')\n",
    "        \n",
    "        grid = dict_grid[algorithm]\n",
    "        clf = GridSearchCV( algorithm ,grid , scoring = '%s' % score, cv = fold)\n",
    "        ind = 0\n",
    "        \n",
    "        for i in fold.test_fold:\n",
    "            if i == 1:\n",
    "                ind = ind + 1\n",
    "        \n",
    "        clf.fit( X, y )\n",
    "        \n",
    "        train_set_X = X[ind:X.shape[0]-ind+1]\n",
    "        train_set_y = y[ind:X.shape[0]-ind+1]\n",
    "        \n",
    "        predictions_train_y = clf.best_estimator_.predict(train_set_X)\n",
    "        train_acc= accuracy_score(train_set_y, predictions_train_y)\n",
    "        \n",
    "        est = estimation (clf.best_estimator_, train_acc ,clf.best_score_, X, y, ind + 1)\n",
    "        models.append( est )\n",
    "            \n",
    " return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best(estimations):\n",
    "    \n",
    "    indices = np.arange(0,len(estimations))\n",
    "    \n",
    "    plot_data_test = np.zeros(len(estimations))\n",
    "    plot_data_train = np.zeros(len(estimations))\n",
    "    i = 0\n",
    "    \n",
    "    for estimation in estimations:\n",
    "        \n",
    "        test_score = estimation.test_score\n",
    "        train_score = estimation.train_score\n",
    "        \n",
    "        plot_data_test[i] = test_score\n",
    "        plot_data_train[i] = train_score\n",
    "        i = i + 1\n",
    "        \n",
    "    plot.figure(figsize= (7,5))\n",
    "    plot.plot(indices, plot_data_test, lw=2, label = 'Test accuracy')\n",
    "    plot.plot(indices, plot_data_train, lw=2, label = 'Train accuracy')\n",
    "    plot.legend()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute all the operations and export the results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from  matplotlib import pyplot as plot\n",
    "\n",
    "import seaborn\n",
    "seaborn.set(style='ticks')\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "\n",
    "#Whether or not to omit the radius & noise features\n",
    "radius_test = False\n",
    "\n",
    "omit = ['TrialNumber','Score']\n",
    "\n",
    "for i in [True]:\n",
    "    radius_test=i\n",
    "    \n",
    "    if not radius_test:\n",
    "        omit.append('Radius')\n",
    "        omit.append('Noise')\n",
    "        \n",
    "    models  = cross_validate( train_model, 'PayOffFunction', omit )\n",
    "    choose_best(models)\n",
    "    \n",
    "    ch = None\n",
    "    score = -999999999\n",
    "    \n",
    "    for i in models:\n",
    "        dist = abs(i.test_score - i.train_score)\n",
    "        if dist > score:\n",
    "            score = dist\n",
    "            ch = i\n",
    "    \n",
    "    R = ch.X.copy()\n",
    "\n",
    "    R['distance'] = ch.model.decision_function(ch.XOriginal)\n",
    "    R['prediction'] = ch.model.predict(ch.XOriginal)\n",
    "    R['label'] = ch.labels\n",
    "    \n",
    "    t = []\n",
    "    \n",
    "    for i in range(0,ch.X.shape[0]):\n",
    "        t.append(i < ch.test_size)\n",
    "        \n",
    "    R['is_test'] = t\n",
    "    \n",
    "    R.to_csv('result_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
