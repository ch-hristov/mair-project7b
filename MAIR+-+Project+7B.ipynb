{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are creating a dataset which is based on all of the dual-task trials per block and we are ignoring the practice trials\n",
    "# The dataset is built by merging the results acquired in the paper created by Chris (---link---)\n",
    "\n",
    "# Some columns are based on the significance provided in the said paper ( & maybe image here)\n",
    "# The csv files we are merging into our dataset are provided below\n",
    "\n",
    "# -MaxDeviationPerTrial.csv\n",
    "# -maxnrDigitEnteredPerPPAndPerBlockPerTrial.csv\n",
    "# -numberOfVisitsTrackerPerParticipantPerBlock.csv (trial data)\n",
    "# samplesOutsideTrial.csv(*)\n",
    "# sdVisTime.csv(*)\n",
    "# tableForMeanTimeInDigitPerPPandPerBlock.csv(*) - merge the data ber block maybe?\n",
    "\n",
    "# We are using the payoff function values provided in the meanVisTime.csv file\n",
    "# The columns marked with star could be significant and we are testing to see the results with or without the said feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'loaded data!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We first read the data and display it\n",
    "final_rows = []\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data.csv');\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "display('loaded data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting length : 8\n"
     ]
    }
   ],
   "source": [
    "# We first split the data into 8 datasets\n",
    "# and we calculate the mean value for each of the dataset\n",
    "# in order to see how biased the data is in terms of \n",
    "# Radius, Noise and the payoff function\n",
    "\n",
    "split_on = ['PayOffFunctionNumeric','Radius','Noise']\n",
    "\n",
    "def split(dataSets,col):\n",
    "    \n",
    "    s = []\n",
    "    \n",
    "    for data in dataSets:\n",
    "        \n",
    "        if(col >= len(split_on)):\n",
    "            return None\n",
    "        \n",
    "        values = {}\n",
    "        \n",
    "        for row in range(0,data.shape[ 0 ]):\n",
    "            splitColumn = split_on[ col ]\n",
    "\n",
    "            rowValue = data.iloc[ row ][ splitColumn ]\n",
    "            \n",
    "            if not rowValue in values:\n",
    "                values[ rowValue ] = [ ]\n",
    "                \n",
    "            values[ rowValue ].append(row)\n",
    "            \n",
    "        for value in values:\n",
    "            \n",
    "            nextSet = []\n",
    "            \n",
    "            for row in values[value]:\n",
    "                nextSet.append(data.iloc[ row ])\n",
    "                \n",
    "            s.append(pd.DataFrame(nextSet))\n",
    "    res = split( s , col + 1 )\n",
    "    \n",
    "    if res is None:\n",
    "        return s\n",
    "    else: \n",
    "        return res\n",
    "    \n",
    "sets = [ data ]\n",
    "\n",
    "split_sets = split(sets,0)\n",
    "\n",
    "print('resulting length : ' +  str(len(split_sets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We then take the top N percent of each of the sets\n",
    "\n",
    "def get_by_split(sets,start, step, end, func):\n",
    "    \n",
    "    while start <= end:\n",
    "        \n",
    "        for dataSet in sets:\n",
    "            \n",
    "            sorted_set = dataSet.sort_values('Score',ascending=[False])\n",
    "            func(start,sorted_set)\n",
    "            \n",
    "        start = start + step\n",
    "\n",
    "per_tick = {}\n",
    "\n",
    "#we save all of the datasets in this variable for later use\n",
    "datasets = []\n",
    "\n",
    "#We then compute the mean for the score column for the provided dataset\n",
    "def mean(current_tick,item):\n",
    "\n",
    "    meanVal = item['Score'].mean()\n",
    "    \n",
    "    if current_tick not in per_tick:\n",
    "        per_tick[current_tick]= []\n",
    "        \n",
    "    per_tick[current_tick].append(meanVal)\n",
    "    datasets.append(item)\n",
    "\n",
    "get_by_split(split_sets, 1, 0.05, 1,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYEhSWpiYEiSmhgYkqQm/w+RnNWu0uaQdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5a58ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can now measure the distributions\n",
    "# by testing different sizes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for tick_size in per_tick:\n",
    "    tick_marks = np.arange(len(per_tick[tick_size]))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(tick_marks,per_tick[tick_size])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 8 datasets\n",
      "starting to aquire chunk 0\n",
      "starting to aquire chunk 1\n",
      "starting to aquire chunk 2\n",
      "starting to aquire chunk 3\n",
      "starting to aquire chunk 4\n",
      "starting to aquire chunk 5\n",
      "starting to aquire chunk 6\n",
      "starting to aquire chunk 7\n",
      "starting to aquire chunk 8\n",
      "starting to aquire chunk 9\n",
      "done partitioning in chunks\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "\n",
    "i = 0;\n",
    "perc = 10\n",
    "rowCounter = 0\n",
    "\n",
    "print( 'there are ' + str(len(datasets)) + ' datasets' )\n",
    "\n",
    "while(i <= 9):\n",
    "    \n",
    "    print('starting to aquire chunk ' + str(i))\n",
    "    \n",
    "    rp = []\n",
    "\n",
    "    for next_set in datasets:\n",
    "        size = int(next_set.shape[0] / perc)\n",
    "        \n",
    "        #starting point\n",
    "        take = i * size\n",
    "        start = take\n",
    "        end = take + size\n",
    "        next_items = next_set[start:end]\n",
    "        \n",
    "        frame = pd.DataFrame(next_items,columns=next_set.columns)\n",
    "        rp.append(frame)\n",
    "        \n",
    "    dictionary[ i ] = pd.concat(rp)\n",
    "    i = i + 1\n",
    "    \n",
    "print('done partitioning in chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then we split the data into training and testing sets by using the train test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def run(func):\n",
    " splits = {}\n",
    " k = 0\n",
    " for k in dictionary:\n",
    "     test_set = dictionary[k].copy()\n",
    "     train_q = []\n",
    "    \n",
    "     for key in dictionary:\n",
    "         if key == k: \n",
    "             continue\n",
    "         train_q.append(dictionary[key])\n",
    "        \n",
    "     tr = pd.concat(train_q)\n",
    "     \n",
    "     y = tr[ 'Score' ]\n",
    "     X = tr.drop( 'Score', 1 )\n",
    "    \n",
    "     y_t = test_set[ 'Score' ]\n",
    "     X_t = test_set.drop('Score',1)\n",
    "    \n",
    "     func(X,y,X_t,y_t)\n",
    "        \n",
    " print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# here be les dragons\n",
    "sv = SVR(cache_size=7000)\n",
    "tr = tree.DecisionTreeRegressor()\n",
    "rfc = RandomForestRegressor()\n",
    "lreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using grid search CV to tweak parameters\n",
    "# We define a hyper parameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dict_grid = {\n",
    "    sv: [\n",
    "            {\n",
    "                'C': [1, 10], \n",
    "                'kernel' : ['linear']\n",
    "            }\n",
    "    ],\n",
    "    tr : [\n",
    "            {\n",
    "                #TODO:...\n",
    "            }\n",
    "    ],\n",
    "    rfc: [\n",
    "        {\n",
    "            'n_estimators' : [5,10],\n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    lreg: [\n",
    "        {\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: -0.498126730443\n",
      "best score: -0.536072857075\n",
      "best score: -1.05472924655\n",
      "best score: -1.46342450515\n",
      "best score: -0.995336076262\n",
      "best score: -1.18407016589\n",
      "best score: -1.21162423148\n",
      "best score: -1.0362719418\n",
      "best score: -1.10459847264\n",
      "best score: -0.836015978981\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "algorithms = [tr]\n",
    "scores = ['r2']\n",
    "\n",
    "def work(X,y,X_t,y_t):\n",
    " for score in scores:\n",
    "    for algorithm in algorithms:\n",
    "        grid = dict_grid[algorithm]\n",
    "        clf = GridSearchCV(algorithm,grid , scoring = '%s' % score)\n",
    "        clf.fit( X, y )\n",
    "        print('best score: ' + str(clf.best_score_))\n",
    "        \n",
    "run(work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
